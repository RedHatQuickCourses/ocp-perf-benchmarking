= Measuring control plane responsiveness with kube-burner

This tutorial walks you through configuring and running kube-burner to measure control plane responsiveness using real latency measurements and Prometheus metrics. You will instrument pod and service latency, generate API load, and analyze the results to detect regressions.


## 1. Understanding Control Plane Metrics
Control plane responsiveness reflects how quickly k8s can:

* Create pods
* Transition pods to ready
* Make services available
* Handle API requests under load

With kube-burner, this is measured using two data sources:

### a. kube-burner Measurements (Built-in)
These measure end-to-end latency:

* `podLatency` → latencies for multiple pod lifecycle events
* `serviceLatency` → Time taken for service to be up and running

They are enabled via:
```
global:
  measurements:
    - name: podLatency
    - name: serviceLatency
```

### b. Prometheus Control Plane Metrics
These provide internal system behavior:

* API server latency
* API request rate (QPS)
* API server CPU usage

These are scraped using a Prometheus metrics configuration.

## 2. Utilizing kube-burner for Control Plane Testing

kube-burner executes high-volume k8s API operations (pod creation, service creation, churn, etc.) while:

* Recording latency measurements
* Scraping Prometheus metrics
* Producing machine-readable JSON output

A typical setup includes:

* A job file (job.yaml) for load generation
* A metrics file (metrics.yaml) for Prometheus scraping
* One or more object templates (pod.yaml, service.yaml)

## 3. Configure kube-burner jobs

### 3.1 Example job.yaml (Control Plane Load Test)
```
global:
  measurements:
    - name: podLatency
    - name: serviceLatency

jobs:
  - name: control-plane-load
    jobType: create
    jobIterations: 1
    qps: 50
    burst: 50

    objects:
      - objectTemplate: pod.yaml
        replicas: 500

      - objectTemplate: service.yaml
        replicas: 50
```
This job:

* Creates 500 pods and 50 services
* Drives load through the k8s API
* Records pod and service latency automatically

### 3.2 Pod Template (pod.yaml)
```
apiVersion: v1
kind: Pod
metadata:
  name: cp-test-pod
  labels:
    app: cp-test
spec:
  containers:
  - name: pause
    image: registry.k8s.io/pause:3.9
```

### 3.3 Service Template (service.yaml)
```
apiVersion: v1
kind: Service
metadata:
  name: cp-test-service
spec:
  selector:
    app: cp-test
  ports:
  - port: 80
    targetPort: 80
```
This is required for serviceLatency to work correctly.

## 4. Prometheus Metrics Configuration
To correlate latency results with control plane behavior, configure Prometheus scraping.

### 4.1 metrics.yaml
```
metricsEndpoints:
- endpoint: http://localhost:9090
  alias: prometheus
  metrics:
  - metrics-control-plane.yaml
  indexer:
    type: local
    metricsDirectory: metrics
```

### 4.2 Control Plane Metrics (metrics-control-plane.yaml)
```
- metricName: apiserverP99
  query: histogram_quantile(0.99,
    sum(rate(apiserver_request_duration_seconds_bucket[2m])) by (le))

- metricName: apiserverQPS
  query: sum(rate(apiserver_request_total[2m]))

- metricName: apiserverCPU
  query: sum(irate(process_cpu_seconds_total{job=~".*apiserver.*"}[2m]))
```
These allow you to correlate:

* Pod latency spikes
* API server latency
* API server CPU saturation

## 5. Run the Test
Execute kube-burner with both configs:

`kube-burner init -c job.yaml -m metrics.yaml`

During execution:

* Pods and services will be created
* Latency will be recorded
* Prometheus metrics will be scraped
* Results will be written to disk

## 6. Analyzing Results
### a. Real pod-latency per-pod document
This is the per-pod document kube-burner indexes for pods that reached `Running`:
```
{
  "timestamp": "2020-11-15T20:28:59.598727718Z",
  "schedulingLatency": 4,
  "initializedLatency": 20,
  "containersReadyLatency": 2997,
  "podReadyLatency": 2997,
  "metricName": "podLatencyMeasurement",
  "uuid": "c40b4346-7af7-4c63-9ab4-aae7ccdd0616",
  "namespace": "kubelet-density",
  "podName": "kubelet-density-13",
  "nodeName": "worker-001",
  "jobName": "create-pods",
  "jobIteration": "2",
  "replica": "3"
}
```
Notes:

* Latencies are in milliseconds.
* `podReadyLatency`` is the end-to-end number you usually care about (API → scheduler → kubelet readiness). 

### b. Pod-latency quantiles
Kube-burner also writes quantiles per condition. Example:
```
{
  "quantileName": "Ready",
  "uuid": "23c0b5fd-c17e-4326-a389-b3aebc774c82",
  "P99": 3774,
  "P95": 3510,
  "P50": 2897,
  "max": 3774,
  "avg": 2876.3,
  "timestamp": "2020-11-15T22:26:51.553221077+01:00",
  "metricName": "podLatencyQuantilesMeasurement"
}
```
Use `P50/P95/P99` for trend/regression detection (P99 is your high-confidence signal). 

All the runtime measurements appear more or less in the similar pattern above.

### c. Prometheus metric documents
Each PromQL result kube-burner writes has this shape with a timestamp.
```
[
  {
    "timestamp": "2021-06-23T11:50:15+02:00",
    "labels": {
      "instance": "ip-10-0-219-170.eu-west-3.compute.internal",
      "mode": "user"
    },
    "value": 0.3300880234732172,
    "uuid": "<UUID>",
    "query": "sum(irate(node_cpu_seconds_total[2m])) by (mode,instance)",
    "metricName": "nodeCPU"
  },
  ...
]
```
kube-burner enriches results with uuid, query, and metricName for correlation. Use metricName to identify which PromQL you ran.

### 7 — Quick checklist: what signals mean what (developer view)

* `podLatencyQuantilesMeasurement P99` (Ready): control-plane slowdown (API/scheduler/kubelet). Check node & apiserver CPU and apiserver request duration. 

* `svcLatencyQuantiles P99`: Endpoints not ready quickly, or network/provider LB delays. Check endpoints and kube-burner-service-latency connectivity logs. 

* `Prometheus apiserver P99`: (PromQL result apiserver_request_duration_seconds quantile) → API server processing time spike. Correlate with achievedQps in jobSummary. 

* `High jobSummary.achievedQps with high latencies`: you pushed cluster beyond what it was tuned for; lower qps/burst for debugging.

Note: The above is just an example, but analyzing metrics and debugging for issues is a wide topic.