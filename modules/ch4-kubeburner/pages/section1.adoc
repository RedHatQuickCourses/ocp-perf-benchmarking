= Measuring control plane responsiveness with kube-burner

This tutorial walks you through configuring and running kube-burner to measure control plane responsiveness using real latency measurements and Prometheus metrics. You will instrument pod and service latency, generate API load, and analyze the results to detect regressions.


== Understanding Control Plane metrics

Control plane responsiveness reflects how quickly k8s can:

 * Create pods
 * Transition pods to ready
 * Make services available
 * Handle API requests under load

With kube-burner, this is measured using two data sources:

=== kube-burner measurements (built-in)

These measure end-to-end latency:

 * `podLatency` → Latencies for multiple pod lifecycle events
 * `serviceLatency` → Time taken for services to be up and running (responsive)

They are enabled via:

```yaml
global:
  measurements:
    - name: podLatency
    - name: serviceLatency
```

=== Prometheus control plane metrics

These provide context about internal system behavior and are scraped using a Prometheus metrics configuration:

 * API server latency
 * API request rate (QPS)
 * API server CPU usage

== Utilizing kube-burner for Control Plane testing

kube-burner executes high-volume k8s API operations (pod creation, service creation, churn, etc.) while:

 * Recording latency measurements
 * Scraping Prometheus metrics
 * Producing machine-readable JSON output

A typical setup includes:

 * A job file (`job.yaml`) for load generation
 * A metrics file (`metrics.yaml`) for Prometheus scraping
 * One or more object templates (I.e.: `pod.yaml`, `service.yaml`, etc.)

== Configure kube-burner jobs

=== Example job for Control Plane load testing

The following example job:

* Creates 500 pods and 50 services
* Drives load through the k8s API
* Records pod and service latency automatically

```yaml
global:
  measurements:
    - name: podLatency
    - name: serviceLatency

jobs:
  - name: control-plane-load
    jobType: create
    jobIterations: 1
    qps: 50
    burst: 50

    objects:
      - objectTemplate: pod.yaml
        replicas: 500

      - objectTemplate: service.yaml
        replicas: 50
```

==== Pod template

Content of the `pod.yaml` file:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: cp-test-pod
  labels:
    app: cp-test
spec:
  containers:
  - name: pause
    image: registry.k8s.io/pause:3.9
```

==== Service template

Content of the `service.yaml` file. This is required for serviceLatency to work correctly:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: cp-test-service
spec:
  selector:
    app: cp-test
  ports:
  - port: 80
    targetPort: 80
```

== Prometheus Metrics Configuration
To correlate latency results with control plane behavior, configure Prometheus scraping.

### Example metrics file

Place this content in the `metrics.yaml` file:

```yaml
metricsEndpoints:
- endpoint: http://localhost:9090
  alias: prometheus
  metrics:
  - metrics-control-plane.yaml
  indexer:
    type: local
    metricsDirectory: metrics
```

### Control Plane metrics

Place this content in the `metrics-control-plane.yaml` file:

```yaml
- metricName: apiserverP99
  query: histogram_quantile(0.99,
    sum(rate(apiserver_request_duration_seconds_bucket[2m])) by (le))

- metricName: apiserverQPS
  query: sum(rate(apiserver_request_total[2m]))

- metricName: apiserverCPU
  query: sum(irate(process_cpu_seconds_total{job=~".*apiserver.*"}[2m]))
```

These allow you to correlate:

* Pod latency spikes
* API server latency
* API server CPU saturation

== Running the test

Execute kube-burner with both configs:

```bash
kube-burner init -c job.yaml -m metrics.yaml
```

During execution:

 * Pods and services will be created
 * Latency will be recorded
 * Prometheus metrics will be scraped
 * Results will be written to disk

== Analyzing Results

=== Pod creation latency per-pod document

This is the per-pod document kube-burner indexes for pods that reached the running state:

```json
{
  "timestamp": "2020-11-15T20:28:59.598727718Z",
  "schedulingLatency": 4,
  "initializedLatency": 20,
  "containersReadyLatency": 2997,
  "podReadyLatency": 2997,
  "metricName": "podLatencyMeasurement",
  "uuid": "c40b4346-7af7-4c63-9ab4-aae7ccdd0616",
  "namespace": "kubelet-density",
  "podName": "kubelet-density-13",
  "nodeName": "worker-001",
  "jobName": "create-pods",
  "jobIteration": "2",
  "replica": "3"
}
```

Notes:

* Latencies are in milliseconds.
* `podReadyLatency`` is the end-to-end number you usually care about (API → scheduler → kubelet readiness).

=== Pod-latency quantiles

Kube-burner also writes quantiles per condition. Example:

```json
{
  "quantileName": "Ready",
  "uuid": "23c0b5fd-c17e-4326-a389-b3aebc774c82",
  "P99": 3774,
  "P95": 3510,
  "P50": 2897,
  "max": 3774,
  "avg": 2876.3,
  "timestamp": "2020-11-15T22:26:51.553221077+01:00",
  "metricName": "podLatencyQuantilesMeasurement"
}
```

Use `P50/P95/P99` for trend/regression detection (P99 is your high-confidence signal).

All the runtime measurements appear more or less in the similar pattern above.

=== Prometheus metric documents

Each PromQL result kube-burner writes has this shape with a timestamp:

```json
[
  {
    "timestamp": "2021-06-23T11:50:15+02:00",
    "labels": {
      "instance": "ip-10-0-219-170.eu-west-3.compute.internal",
      "mode": "user"
    },
    "value": 0.3300880234732172,
    "uuid": "<UUID>",
    "query": "sum(irate(node_cpu_seconds_total[2m])) by (mode,instance)",
    "metricName": "nodeCPU"
  },
  ...
]
```

kube-burner enriches results with uuid, query, and metricName for correlation. Use metricName to identify which PromQL you ran.

=== Quick checklist: signal meanings for developers

* *High P99 pod ready latency quantiles measurement*: control-plane slowdown (API/scheduler/kubelet). Check node and OpenShift API server CPU usage and API server request duration.

* *High P99 service latency quantiles*: Endpoints not ready quickly, or network/provider load balancer delays. Check endpoints and kube-burner-service-latency connectivity logs.

* *High Prometheus API server P99*: (PromQL result apiserver_request_duration_seconds quantile) → API server processing time spike. Correlate with `achievedQps` in `jobSummary`.

* *High jobSummary.achievedQps with high latencies*: the cluster has been pushed beyond what it was tuned for; lower QPS/burst for debugging.

NOTE: The above is just an example, analyzing metrics and debugging for issues is a wide topic very specific to the subsystem being analyzed.