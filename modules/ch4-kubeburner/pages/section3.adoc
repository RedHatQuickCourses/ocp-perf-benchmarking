= Hands-on with kube-burner

In this section, we will walk through a practical hands-on activity that illustrates how to use kube-burner for simulating cluster churn and measuring Control Plane responsiveness. This exercise assumes you have basic familiarity with Kubernetes and its command-line interface (kubectl).

== Prerequisites:

1. Install `kube-burner` binary or use the container image on your preferred system. Refer to the [installation guide](https://cloud-bulldozer.github.io/kube-burner/) for more details.
2. Access a running Kubernetes cluster (either local or cloud-based).
3. Ensure `kubectl` is configured and can communicate with your target Kubernetes cluster.

=== Install kube-burner-ocp

```bash
curl -Ls https://raw.githubusercontent.com/kube-burner/kube-burner-ocp/refs/heads/main/hack/install.sh | sh
```

Validate the installation:
```bash
kube-burner-ocp help
```

== Objectives

By the end of this lab activity, you should be able to:
 * Configure kube-burner jobs to simulate various control plane churn scenarios.
 * Measure and observe the control plane's responsiveness under stress.
 * Analyze test results and interpret key performance metrics.

== Step-by-step Hands-on Activity

=== Setting up kube-burner configuration files

kube-burner uses YAML configuration files to define jobs, which dictate the resources it will create, delete, or manipulate in your Kubernetes cluster. You can find example configuration files and metrics profiles in the [kube-burner examples directory](https://github.com/cloud-bulldozer/kube-burner/tree/master/examples).

Let's start by creating a simple `job.yaml` file to simulate control plane churn:

```yaml
apiVersion: kubeburner.cloud-bulldozer.io/v1alpha1
kind: Job
metadata:
  name: controlplane-churn-job
spec:
  # Specify the type of resource you want to test
  resources:
    - kind: Deployment
  # Define the number of operations for each kind of resource
  operations:
    - name: Create
      count: 10
    - name: Update
      count: 10
    - name: Delete
      count: 10
```

This configuration instructs kube-burner to create, update, and delete 10 Kubernetes Deployments. Adjust the `count` values as needed for your specific testing requirements.

=== Running kube-burner job

Now that we have a configuration file ready, let's run our job against the target Kubernetes cluster:

```bash
kubectl apply -f job.yaml
```

kube-burner will start creating, updating, and deleting Deployments according to your `job.yaml` specifications. You can monitor the progress using:

```bash
kubectl -n kube-burner logs <pod-name>
```

Replace `<pod-name>` with the name of the pod generated by kube-burner for executing the job.

=== Measuring Control Plane Responsiveness

While the job is running, kube-burner will collect Prometheus metrics related to control plane components (kube-apiserver, etcd, etc.). To view these metrics, set up Grafana with a predefined dashboard or use `curl` to fetch metrics directly from the Kube-API server:

```bash
kubectl top node <node-name> --format=json | jq '.containers[] | select(.name=="kube-apiserver") | .metrics'
```

Replace `<node-name>` with the name of a node in your cluster running kube-apiserver. This command extracts and displays metrics related to kube-apiserver performance.

=== Analyzing Results

Once the job completes, analyze the collected Prometheus metrics to evaluate control plane responsiveness under stress. Key metrics to observe include:

- Request latency
- Error rates (5xx)
- Throughput

You can find more information on interpreting these metrics in kube-burner's documentation or through subsequent blog posts (as mentioned in the provided context).

=== Indexing results in ElasticSearch

1. Get ElasticSearch password:

+
[source,bash]
----
ES_PASSWORD=$(oc get secret elasticsearch-es-elastic-user -n openshift-logging -o jsonpath='{.data.elastic}' | base64 -d)
----

2. Get ElasticSearch route URL:

+
[source,bash]
----
ES_INSTANCE=$(oc get route elasticsearch-route -n openshift-logging -o jsonpath="https://elastic:${ES_PASSWORD}@{.spec.host}")
----

3. Validate ElasticSearch authentication:

+
[source,bash]
----
curl -k $ES_INSTANCE
----
+
[source,json]
----
{
  "name" : "elasticsearch-es-default-0",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "gfP33bmwQ82UMWI2btj2OQ",
  "version" : {
    "number" : "8.15.0",
    "build_flavor" : "default",
    "build_type" : "docker",
    "build_hash" : "1a77947f34deddb41af25e6f0ddb8e830159c179",
    "build_date" : "2024-08-05T10:05:34.233336849Z",
    "build_snapshot" : false,
    "lucene_version" : "9.11.1",
    "minimum_wire_compatibility_version" : "7.17.0",
    "minimum_index_compatibility_version" : "7.0.0"
  },
  "tagline" : "You Know, for Search"
}
----

4. Extract the node-density workload so we can change the default metrics endpoint from `opensearch` to `elastic`:

+
[source,bash]
----
mkdir node-density
cd node-density/
kube-burner-ocp node-density --extract
----

5. Let's take a look at the files that compose the workload:

+
[source,bash]
----
tree
----
+
[source]
----
.
â”œâ”€â”€ alerts.yml
â”œâ”€â”€ metrics-aggregated.yml
â”œâ”€â”€ metrics-report.yml
â”œâ”€â”€ metrics.yml
â”œâ”€â”€ node-density.yml
â””â”€â”€ pod.yml
----

 * `alerts.yml`: Alert file
 * `metrics-report.yml`: Aggregated metrics
 * `metrics.yml`: Instant metrics
 * `node-density.yml`: The workload definition
 * `pod.yml`: The pod definition

6. Change the default metrics endpoint from `opensearch` to `elastic`:

+
[source,bash]
----
sed -i 's/opensearch/elastic/g' node-density.yml
----

7. Run a small node-density workload and index results on the ElasticSearch instance:

+
[source,bash]
----
kube-burner-ocp node-density --pods-per-node 75 --es-server $ES_INSTANCE --es-index kube-burner
----
+
----
time="2025-12-03 08:35:17" level=info msg="â¤ï¸ Checking for Cluster Health" file="cluster-health.go:46"
time="2025-12-03 08:35:17" level=info msg="Cluster is Healthy" file="cluster-health.go:54"
time="2025-12-03 08:35:17" level=info msg="Config file node-density.yml available in the current directory, using it" file="file_reader.go:77"
time="2025-12-03 08:35:17" level=info msg="ðŸ“ Creating elastic indexer: indexer-0" file="metrics.go:67"
time="2025-12-03 08:35:18" level=info msg="ðŸ‘½ Initializing prometheus client with URL: https://prometheus-k8s-openshift-monitoring.apps.cluster-vjm62.dynamic.redhatworkshops.io" file="prometheus.go:45"
time="2025-12-03 08:35:18" level=info msg="Config file metrics.yml available in the current directory, using it" file="file_reader.go:77"
time="2025-12-03 08:35:18" level=info msg="Config file metrics-report.yml available in the current directory, using it" file="file_reader.go:77"
time="2025-12-03 08:35:18" level=info msg="ðŸ”” Initializing alert manager for prometheus: https://prometheus-k8s-openshift-monitoring.apps.cluster-vjm62.dynamic.redhatworkshops.io" file="alert_manager.go:91"
time="2025-12-03 08:35:18" level=info msg="Config file alerts.yml available in the current directory, using it" file="file_reader.go:77"
time="2025-12-03 08:35:18" level=info msg="ðŸ”¥ Starting kube-burner (1.8.1@feeba2e8ce333cf69862208df78ec6a916096b14) with UUID bfd5d1d4-029f-4dc2-9968-86b857f45137" file="job.go:90"
time="2025-12-03 08:35:18" level=info msg="Config file pod.yml available in the current directory, using it" file="file_reader.go:77"
time="2025-12-03 08:35:18" level=info msg="Pre-load: images from job node-density" file="pre_load.go:73"
time="2025-12-03 08:35:18" level=info msg="Pre-load: Creating DaemonSet using images [registry.k8s.io/pause:3.1] in namespace preload-kube-burner" file="pre_load.go:195"
time="2025-12-03 08:35:18" level=info msg="Pre-load: Sleeping for 10s" file="pre_load.go:86"
time="2025-12-03 08:35:28" level=info msg="Deleting 1 namespaces with label: kube-burner-preload=true" file="namespaces.go:66"
time="2025-12-03 08:35:39" level=info msg="Initializing measurements for job: node-density" file="factory.go:99"
time="2025-12-03 08:35:39" level=info msg="Registered measurement: podLatency" file="factory.go:134"
time="2025-12-03 08:35:39" level=info msg="Creating /v1, Resource=pods latency watcher for node-density" file="base_measurement.go:69"
time="2025-12-03 08:35:39" level=info msg="Triggering job: node-density" file="job.go:121"
time="2025-12-03 08:35:39" level=info msg="6/66 iterations completed" file="create.go:116"
time="2025-12-03 08:35:40" level=info msg="30/66 iterations completed" file="create.go:116"
time="2025-12-03 08:35:41" level=info msg="60/66 iterations completed" file="create.go:116"
time="2025-12-03 08:35:42" level=info msg="Waiting up to 4h0m0s for actions to be completed" file="create.go:169"
time="2025-12-03 08:35:46" level=info msg="Actions in namespace node-density-0 completed" file="waiters.go:74"
time="2025-12-03 08:35:46" level=info msg="Verifying created objects" file="utils.go:137"
time="2025-12-03 08:35:46" level=info msg="Job node-density took 7s" file="job.go:190"
time="2025-12-03 08:35:46" level=info msg="Stopping measurement: podLatency" file="factory.go:163"
time="2025-12-03 08:35:46" level=info msg="Evaluating latency thresholds" file="metrics.go:48"
time="2025-12-03 08:35:46" level=info msg="node-density: PodReadyToStartContainers 99th: 0 max: 0 avg: 0" file="base_measurement.go:111"
time="2025-12-03 08:35:46" level=info msg="node-density: PodScheduled 99th: 0 max: 0 avg: 0" file="base_measurement.go:111"
time="2025-12-03 08:35:46" level=info msg="node-density: ContainersReady 99th: 4000 max: 4000 avg: 2833" file="base_measurement.go:111"
time="2025-12-03 08:35:46" level=info msg="node-density: Initialized 99th: 1000 max: 1000 avg: 30" file="base_measurement.go:111"
time="2025-12-03 08:35:46" level=info msg="node-density: Ready 99th: 4000 max: 4000 avg: 2833" file="base_measurement.go:111"
time="2025-12-03 08:35:46" level=info msg="Indexing collected data from measurement: podLatency" file="factory.go:175"
time="2025-12-03 08:35:46" level=info msg="Indexing metric podLatencyMeasurement" file="base_measurement.go:134"
time="2025-12-03 08:35:46" level=info msg="Deleting Pods labeled with kube-burner-job=node-density in node-density-0" file="namespaces.go:55"
time="2025-12-03 08:35:46" level=info msg="Indexing finished in 361ms: created=66" file="base_measurement.go:143"
time="2025-12-03 08:35:46" level=info msg="Indexing metric podLatencyQuantilesMeasurement" file="base_measurement.go:134"
time="2025-12-03 08:35:46" level=info msg="Indexing finished in 80ms: created=5" file="base_measurement.go:143"
time="2025-12-03 08:35:46" level=info msg="Evaluating alerts for job node-density in: https://prometheus-k8s-openshift-monitoring.apps.cluster-vjm62.dynamic.redhatworkshops.io" file="alert_manager.go:126"
time="2025-12-03 08:35:47" level=info msg="Indexing alerts" file="alert_manager.go:222"
time="2025-12-03 08:35:47" level=info msg="Indexing finished in 98ms: created=3 redundantskipped=1" file="alert_manager.go:228"
time="2025-12-03 08:35:47" level=info msg="Indexing job summaries" file="metadata.go:48"
time="2025-12-03 08:35:47" level=info msg="Indexing finished in 104ms: created=1" file="metadata.go:65"
time="2025-12-03 08:35:47" level=info msg="ðŸ” Endpoint: https://prometheus-k8s-openshift-monitoring.apps.cluster-vjm62.dynamic.redhatworkshops.io; profile: metrics.yml start: 2025-12-03T13:35:39Z end: 2025-12-03T13:35:46Z; job: node-density, metricsClosing: afterJobPause" file="prometheus.go:68"
time="2025-12-03 08:35:48" level=info msg="ðŸ” Endpoint: https://prometheus-k8s-openshift-monitoring.apps.cluster-vjm62.dynamic.redhatworkshops.io; profile: metrics-report.yml start: 2025-12-03T13:35:39Z end: 2025-12-03T13:35:46Z; job: node-density, metricsClosing: afterJobPause" file="prometheus.go:68"
time="2025-12-03 08:35:51" level=info msg="Indexing [331] documents from metric APIRequestRate" file="prometheus.go:222"
time="2025-12-03 08:35:51" level=info msg="Indexing finished in 174ms: created=331" file="prometheus.go:227"
time="2025-12-03 08:35:51" level=info msg="Indexing [662] documents from metric containerCPU" file="prometheus.go:222"
time="2025-12-03 08:35:51" level=info msg="Indexing finished in 172ms: created=662" file="prometheus.go:227"
time="2025-12-03 08:35:52" level=info msg="Indexing [738] documents from metric containerMemory" file="prometheus.go:222"
time="2025-12-03 08:35:52" level=info msg="Indexing finished in 164ms: created=738" file="prometheus.go:227"
time="2025-12-03 08:35:52" level=info msg="Finished execution with UUID: bfd5d1d4-029f-4dc2-9968-86b857f45137" file="job.go:263"
time="2025-12-03 08:35:52" level=info msg="Garbage collecting jobs" file="job.go:290"
time="2025-12-03 08:35:53" level=info msg="Deleting 1 namespaces with label: kube-burner-job=node-density" file="namespaces.go:66"
time="2025-12-03 08:36:05" level=info msg="ðŸ‘‹ kube-burner run completed with rc 0 for UUID bfd5d1d4-029f-4dc2-9968-86b857f45137" file="helpers.go:105"
----

=== Visualization of results in Grafana

Let's create the *Kube-burner Report* dashboard in Grafana and visualize the indexed data.

1. Download the latest version of the dashboard:

+
[source,bash]
----
wget https://raw.githubusercontent.com/kube-burner/kube-burner-ocp/refs/heads/main/dashboards/Kube-burner.json
----

2. Grafana does NOT accept a raw dashboard JSON file, it requires a wrapper object around it:

+
[source,bash]
----
jq -n --argfile dash Kube-burner.json '{
  dashboard: $dash,
  overwrite: true,
  folderId: 0
}' > Kube-burner-wrapped.json
----

3. Finally, let's create the dashboard:

+
[source,bash]
----
curl -X POST -sS --insecure \
    -u admin:admin123 \
    -H "Content-Type: application/json" \
    -d @Kube-burner-wrapped.json \
    https://grafana-route-openshift-logging.apps.cluster-vjm62.dynamic.redhatworkshops.io/api/dashboards/import | jq .
----
+
[source,json]
----
{
  "uid": "dbcfe71e",
  "pluginId": "",
  "title": "Kube-burner Report",
  "imported": true,
  "importedUri": "db/kube-burner-report",
  "importedUrl": "/d/dbcfe71e/kube-burner-report",
  "slug": "kube-burner-report",
  "dashboardId": 3,
  "folderId": 0,
  "folderUid": "",
  "description": "",
  "path": "",
  "removed": false
}
----

4. Let's make sure the ElasticSearch data source is properly configured:

+
.ElasticSearch setup
image::grafana-es-config.png[width=450]

5. Let's visualize the report in the newly created *Kube-burner Report* dashboard:

+
.Kube-burner Report dashboard
image::grafana-kb.png[width=700]

== Conclusion

This hands-on activity has demonstrated how to configure and run kube-burner jobs for simulating control plane churn and measuring responsiveness. Through practical exercises like this, users can gain valuable insights into Kubernetes cluster performance under stress conditions. Remember to consult the comprehensive [kube-burner documentation](https://cloud-bulldozer.github.io/kube-burner/) for more in-depth guidance on using kube-burner's features and understanding test results.
