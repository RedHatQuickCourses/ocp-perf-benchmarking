= Introduction to Performance Benchmarking

== Overview

This module introduces the fundamentals of performance engineering within computer networks, focusing on how benchmarking provides measurable, repeatable insights into system behavior under load. It explains why modern infrastructures—spanning physical, virtualized, and cloud-native components—require systematic performance evaluation to validate design choices, uncover bottlenecks, and ensure reliability at scale.

== What you'll learn

In this section, you will learn:

 * The role of performance benchmarking in evaluating network throughput, latency, jitter, and scalability
 * How realistic traffic patterns and controlled test scenarios help reveal bottlenecks and validate design decisions
 * Why performance engineering must account for cloud-native and virtualized components such as SDN controllers, service meshes, and microservices
 * The importance of repeatability in benchmarking and how standardized methodologies ensure consistent, meaningful results
 * How automation and CI/CD-driven pipelines enable continuous performance validation and regression detection


== Performance engineering

Performance benchmarking in computer networks is the systematic process of measuring, comparing, and evaluating how network components behave under specific workloads. It provides engineers, operators, and architects with quantifiable evidence of performance characteristics such as throughput, latency, jitter, and resource utilization. Rather than relying on intuition or vendor claims, benchmarking establishes repeatable, data-driven insights that help determine whether a network meets its intended service objectives.

One of the core goals of performance benchmarking is to simulate *realistic traffic patterns* and operational conditions. Networks experience a wide range of workloads—from bursty interactive flows to sustained high-bandwidth transfers—and each stresses the system differently. Benchmarking tools and methodologies attempt to mimic these behaviors so that engineers can understand how devices such as switches, routers, load balancers, or virtualized network functions respond under stress. This allows teams to uncover bottlenecks that may remain invisible during normal operation.

Benchmarking also helps validate network design decisions before they are deployed into production. For example, performance tests can verify whether a new routing protocol configuration improves convergence times, whether a firewall can sustain expected peak loads, or whether a virtualized network function performs comparably to its hardware-based equivalent. By measuring performance early, organizations reduce deployment risks and avoid costly redesigns later in the lifecycle.

In modern infrastructures, benchmarking extends beyond physical devices to include *virtualized and cloud-native network components*. Technologies such as SDN controllers, service meshes, and microservice-based load balancers introduce new performance considerations. Their behavior often depends not only on the network but also on compute, storage, and orchestration layers. As a result, performance benchmarking in contemporary environments must adopt a holistic approach that captures system-wide interactions.

=== Repeatability

A key aspect of effective benchmarking is *repeatability*. Results must be consistent across multiple runs, environments, and configurations. To achieve this, benchmarks rely on controlled testbeds, standardized procedures, and well-defined test parameters. Repeatable tests ensure that performance deviations are meaningful and reflect true system behavior rather than noise.

=== Automation

*Automation* plays an increasingly important role in benchmarking practices. Continuous performance testing pipelines, integrated into CI/CD workflows, allow engineers to detect performance regressions as part of routine software development. Automated test harnesses can deploy environments, generate traffic, collect telemetry, and analyze results without manual intervention. This shift enables teams to evaluate performance at scale and at the pace demanded by modern development cycles.

== Next steps

After completing this module, you'll be ready to:

 * Explore specific benchmarking tools and suites for networking, compute, and storage
 * Learn how to design repeatable, automated benchmarking pipelines
 * Apply these concepts to Kubernetes and OpenShift environments, where cloud-native architectures introduce new performance considerations

== See also

* link:https://www.redhat.com/en/authors/red-hat-performance-team[Red Hat Performance Team]
* link:https://www.redhat.com/en/blog/red-hat-performance-and-scale-engineering[Red Hat Performance and Scale Engineering]